{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "106911b2",
      "metadata": {
        "id": "106911b2",
        "outputId": "e81d6ba8-141a-4924-8a5e-010437aece95"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PROJ: proj_create_from_database: Cannot find proj.db\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import  math\n",
        "from statistics import mean\n",
        "from statistics import stdev\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "from scipy.stats import norm\n",
        "import statistics as stat\n",
        "from scipy.stats import pearsonr\n",
        "import random\n",
        "import contextily as ctx\n",
        "import seaborn.palettes\n",
        "import seaborn.utils\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities\n",
        "\n",
        "This document has three utility function\n",
        "\n",
        "---\n",
        "\n",
        "### 1. `calculate_pvalues(df)`\n",
        "- **Purpose:**  \n",
        "  Calculates p-values for pairwise Pearson correlation coefficients between numeric columns in a DataFrame.  \n",
        "- **Why Created:**  \n",
        "  To assess the statistical significance of correlations in numeric datasets, enabling data analysts to identify relationships that are not due to random chance.  \n",
        "- **Usage:**  \n",
        "  Useful for exploratory data analysis to identify meaningful associations between variables. Only numeric columns are analyzed, and missing values are handled by row exclusion.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. `mean_std(data)`\n",
        "- **Purpose:**  \n",
        "  Computes the mean and standard deviation of a flattened 2D data structure.  \n",
        "- **Why Created:**  \n",
        "  Provides a quick statistical summary of 2D numerical datasets, which is valuable for understanding the central tendency and variability in data.  \n",
        "- **Usage:**  \n",
        "  Applied during data preprocessing or exploratory analysis, particularly for normalized or structured data stored in 2D lists.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. `ProcessData(path)`\n",
        "- **Purpose:**  \n",
        "  Loads and preprocesses a dataset containing location-specific high and low variable values.  \n",
        "- **Why Created:**  \n",
        "  Facilitates the ingestion of spatial and categorical datasets, converting them into a structured pandas DataFrame for further analysis. This is especially valuable for co-location pattern studies.  \n",
        "- **Usage:**  \n",
        "  Parses a CSV file containing geospatial data, standardizes column headers, and ensures appropriate data types for latitude, longitude, and variable values. Designed for workflows involving spatial data analysis and co-location mining.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "u5SXrXAxBBES"
      },
      "id": "u5SXrXAxBBES"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c99fe87",
      "metadata": {
        "id": "3c99fe87"
      },
      "outputs": [],
      "source": [
        "def calculate_pvalues(df):\n",
        "    \"\"\"\n",
        "    Calculate the p-values for pairwise Pearson correlation coefficients between numeric columns in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): The input DataFrame containing numeric data.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: A DataFrame containing the p-values of pairwise Pearson correlations.\n",
        "                       Each cell [r, c] represents the p-value for the correlation between\n",
        "                       column r and column c in the input DataFrame.\n",
        "\n",
        "    Notes:\n",
        "    - The function drops rows with missing values before computation.\n",
        "    - Only numeric columns are considered for the analysis.\n",
        "    \"\"\"\n",
        "    # Drop rows with missing values and select only numeric columns\n",
        "    df = df.dropna()._get_numeric_data()\n",
        "\n",
        "    # Create an empty DataFrame to store p-values\n",
        "    dfcols = pd.DataFrame(columns=df.columns)\n",
        "    pvalues = dfcols.transpose().join(dfcols, how='outer')\n",
        "\n",
        "    # Calculate p-values for each pair of columns\n",
        "    for r in df.columns:\n",
        "        for c in df.columns:\n",
        "            pvalues[r][c] = round(pearsonr(df[r], df[c])[1], 4)\n",
        "\n",
        "    return pvalues\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5efd19a1",
      "metadata": {
        "id": "5efd19a1"
      },
      "outputs": [],
      "source": [
        "def mean_std(data):\n",
        "    \"\"\"\n",
        "    Calculate the mean and standard deviation of a flattened 2D data structure.\n",
        "\n",
        "    Parameters:\n",
        "    data (list of lists): A 2D list containing numerical values.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing:\n",
        "        - datamean (float): The mean of the flattened data.\n",
        "        - datastd (float): The standard deviation of the flattened data.\n",
        "    \"\"\"\n",
        "    # Flatten the 2D data into a single list\n",
        "    flat_data = [element for item in data for element in item]\n",
        "\n",
        "    # Calculate mean and standard deviation\n",
        "    datastd = stat.stdev(flat_data)\n",
        "    datamean = stat.mean(flat_data)\n",
        "\n",
        "    return datamean, datastd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cf59b7ae",
      "metadata": {
        "id": "cf59b7ae"
      },
      "outputs": [],
      "source": [
        "def ProcessData(path):\n",
        "    \"\"\"\n",
        "    Load and preprocess a dataset containing locationwise high and low variable values.\n",
        "\n",
        "    Parameters:\n",
        "    path (str): The file path to the CSV file to be processed.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: A DataFrame containing the processed co-location data with specified column headers and data types.\n",
        "\n",
        "    Column Details:\n",
        "    - Zip (str): Identifier for the location (e.g., zip code).\n",
        "    - lat (float64): Latitude values.\n",
        "    - lon (float64): Longitude values.\n",
        "    - high (float64): Values for the high variable.\n",
        "    - low (float64): Values for the low variable.\n",
        "    \"\"\"\n",
        "    # Define column headers and data types for the dataset\n",
        "    headers = ['Zip', 'lat', 'lon', 'high', 'low']\n",
        "    dtypes = {'Zip': 'str', 'lat': 'float64', 'lon': 'float64', 'high': 'float64', 'low': 'float64'}\n",
        "\n",
        "    # Load the CSV file into a DataFrame with the specified headers and data types\n",
        "    data = pd.read_csv(path, names=headers, dtype=dtypes)\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Implementation Functions Summary\n",
        "\n",
        "This summary outlines the implementation of key spatial analysis functions designed for influence density calculation and co-location analysis. These functions provide foundational tools for spatial pattern discovery and analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. `influence(data, pattern, xx, yy, bandwidth, n)`\n",
        "- **Purpose:**  \n",
        "  Calculates the influence density of a given pattern over a spatial grid using Gaussian Kernel Density Estimation (KDE).\n",
        "- **Parameters:**  \n",
        "  - `data`: List of data points with spatial coordinates and pattern values.  \n",
        "  - `pattern`: Index representing the pattern value in the data points.  \n",
        "  - `xx`, `yy`: 2D grids of longitudes and latitudes for density evaluation.  \n",
        "  - `bandwidth`: Bandwidth parameter for smoothing in Gaussian KDE.  \n",
        "  - `n`: Total number of data points (recalculated internally).  \n",
        "- **Output:**  \n",
        "  - `density_2d`: A 2D list of influence densities for each grid point.  \n",
        "- **Key Note:**  \n",
        "  Useful for detecting spatial hotspots of a specific pattern using density evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. `influenceO(data, pattern, xx, yy, bandwidth, n)` (Hypothetical Comparison)\n",
        "- **Key Difference from `influence`:**  \n",
        "  - While `influence` uses Gaussian KDE, `influenceO` may employ an alternative way of calculation.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. `equationC3NoLog(data1, data2, data3, xdimention, ydimention, xx, yy)`\n",
        "- **Purpose:**  \n",
        "  Calculates co-location values across a grid using normalized Z-scores for three spatial density datasets.\n",
        "- **Parameters:**  \n",
        "  - `data1`, `data2`, `data3`: 2D density data for three spatial variables.  \n",
        "  - `xdimention`, `ydimention`: Dimensions of the spatial grid.  \n",
        "  - `xx`, `yy`: 2D grids of longitudes and latitudes for analysis.  \n",
        "- **Output:**  \n",
        "  - Co-location values represented as a 2D grid, highlighting areas where the three variables exhibit significant interaction or association.  \n",
        "- **Usage:**  \n",
        "  Useful for multivariate spatial analysis to identify regions with complex co-location patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. `equationC2NoLog(data1, data2, xdimention, ydimention, xx, yy)`\n",
        "- **Purpose:**  \n",
        "  Calculates co-location values for two spatial density datasets using normalized Z-scores.\n",
        "- **Parameters:**  \n",
        "  - `data1`, `data2`: 2D density data for two spatial variables.  \n",
        "  - `xdimention`, `ydimention`: Dimensions of the spatial grid.  \n",
        "  - `xx`, `yy`: 2D grids of longitudes and latitudes for analysis.  \n",
        "- **Output:**  \n",
        "  - Co-location values as a 2D grid, illustrating pairwise spatial interaction between two variables.  \n",
        "- **Usage:**  \n",
        "  Focuses on pairwise spatial associations, offering simpler but more focused spatial analysis compared to `equationC3NoLog`.\n",
        "\n",
        "---\n",
        "### 5. `generate_sample_points(data)`\n",
        "- **Purpose:**  \n",
        "  Generates a grid of sample points within the bounding box defined by the dataset's minimum and maximum longitude and latitude values.\n",
        "- **Parameters:**  \n",
        "  - `data`: A dictionary containing keys `'lon'` and `'lat'`, each associated with a list of longitude and latitude values, respectively.  \n",
        "- **Output:**  \n",
        "  - `xx`: 2D array of longitude values representing the grid.  \n",
        "  - `yy`: 2D array of latitude values representing the grid.  \n",
        "  - `grid`: A list of `[longitude, latitude]` pairs for each grid point.  \n",
        "- **Key Note:**  \n",
        "  Forms the foundational grid for spatial density and co-location analysis.\n",
        "  ---\n",
        "### Key Differences\n",
        "- **`influence` vs `influenceO`:**  \n",
        "  The main difference lies in the kernel estimation methods or weighting mechanisms. `influence` uses Gaussian KDE, while `influenceO` uses an alternative way of calculation.\n",
        "- **`equationC3NoLog` vs `equationC2NoLog`:**  \n",
        "  `equationC3NoLog` evaluates co-locations for three variables simultaneously, highlighting complex spatial interactions. `equationC2NoLog` focuses on simpler pairwise co-locations.\n",
        "\n"
      ],
      "metadata": {
        "id": "xrjrOU39Cayq"
      },
      "id": "xrjrOU39Cayq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e21f64b",
      "metadata": {
        "id": "7e21f64b"
      },
      "outputs": [],
      "source": [
        "def influence(data, pattern, xx, yy, bandwidth, n):\n",
        "    \"\"\"\n",
        "    Calculate the influence density of a given pattern across a spatial grid using Gaussian kernel density estimation.\n",
        "\n",
        "    Parameters:\n",
        "    data (list): A list of data points, where each data point contains spatial coordinates and pattern values.\n",
        "    pattern (int): Index of the pattern value in the data points.\n",
        "    xx (list of lists): 2D grid of longitudes for density evaluation.\n",
        "    yy (list of lists): 2D grid of latitudes for density evaluation.\n",
        "    bandwidth (float): Bandwidth parameter for Gaussian kernel smoothing.\n",
        "    n (int): Total number of data points (redundant as it is recalculated internally).\n",
        "\n",
        "    Returns:\n",
        "    density_2d (list of lists): A 2D list containing influence densities at each grid point.\n",
        "\n",
        "    \"\"\"\n",
        "    # Recalculate the number of data points (n) from the input dataset\n",
        "    n = len(data)\n",
        "\n",
        "    # Initialize variables for density calculation\n",
        "    xcount = 0\n",
        "    density_2d = []  # 2D list to store density values\n",
        "    x = []  # List to store longitude values\n",
        "    y = []  # List to store latitude values\n",
        "    d = []  # List to store calculated density values\n",
        "\n",
        "    # Iterate through the longitude grid\n",
        "    for longitude in xx:\n",
        "        temp = []  # Temporary list to store density values for the current longitude row\n",
        "        ycount = 0  # Latitude index\n",
        "\n",
        "        # Iterate through the latitude grid\n",
        "        for latitude in yy:\n",
        "            sum_probability = 0  # Sum of weighted probabilities for the current grid point\n",
        "\n",
        "            # Store grid coordinates\n",
        "            x.append(xx[xcount][xcount])\n",
        "            y.append(yy[ycount][ycount])\n",
        "\n",
        "            # Calculate the influence density for the current grid point\n",
        "            for item in data:\n",
        "                # Compute squared distances in longitude and latitude\n",
        "                xdistance = (item[1] - longitude[xcount]) ** 2\n",
        "                ydistance = (item[0] - latitude[ycount]) ** 2\n",
        "\n",
        "                # Compute combined distance and apply Gaussian kernel formula\n",
        "                combined_distance = xdistance + ydistance\n",
        "                probability = math.exp(-1 * combined_distance / (2 * bandwidth ** 2))\n",
        "\n",
        "                # Weight the probability by the pattern value\n",
        "                weighted_probability = item[pattern] * probability\n",
        "                sum_probability += weighted_probability\n",
        "\n",
        "            # Normalize the probability by the kernel area and total number of points\n",
        "            kernel_area = 2 * math.pi * bandwidth ** 2\n",
        "            average_probability = sum_probability / (n * kernel_area)\n",
        "\n",
        "            # Append the density value\n",
        "            d.append(average_probability)\n",
        "            temp.append(average_probability)\n",
        "            ycount += 1\n",
        "\n",
        "        # Append the row of density values to the 2D density list\n",
        "        density_2d.append(temp)\n",
        "        xcount += 1\n",
        "\n",
        "    # Create a dictionary to store the results in tabular format\n",
        "    density_dict = {'X': x, 'Y': y, 'Colocation': d}\n",
        "\n",
        "    # Convert the dictionary to a Pandas DataFrame and save it to a CSV file\n",
        "    #df = pd.DataFrame(data=density_dict)\n",
        "    #df.to_csv('UnemploymentLow.csv', index=False)\n",
        "\n",
        "    # Return the 2D density grid\n",
        "    return density_2d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e282c633",
      "metadata": {
        "id": "e282c633"
      },
      "outputs": [],
      "source": [
        "def influenceO(data, pattern, xx, yy, bandwidth):\n",
        "    \"\"\"\n",
        "    Calculate the influence density of a given pattern across a spatial grid using Gaussian kernel density estimation.\n",
        "\n",
        "    Parameters:\n",
        "    data (list): A list of data points, where each data point contains spatial coordinates and pattern values.\n",
        "    pattern (int): Index of the pattern value in the data points.\n",
        "    xx (list of lists): 2D grid of longitudes for density evaluation.\n",
        "    yy (list of lists): 2D grid of latitudes for density evaluation.\n",
        "    bandwidth (float): Bandwidth parameter for Gaussian kernel smoothing.\n",
        "\n",
        "    Returns:\n",
        "    density_2d (list of lists): A 2D list containing influence densities at each grid point.\n",
        "    \"\"\"\n",
        "    # Recalculate the number of data points (n) from the input dataset\n",
        "    n = len(data)\n",
        "\n",
        "    # Precompute constants for efficiency\n",
        "    kernel_area = 2 * math.pi * bandwidth ** 2\n",
        "\n",
        "    # Initialize variables for density calculation\n",
        "    density_2d = []  # 2D list to store density values\n",
        "\n",
        "    # Iterate through the longitude grid\n",
        "    for longitude_row in xx:\n",
        "        temp = []  # Temporary list to store density values for the current longitude row\n",
        "\n",
        "        # Iterate through the latitude grid\n",
        "        for latitude_row in yy:\n",
        "            sum_probability = 0  # Sum of weighted probabilities for the current grid point\n",
        "\n",
        "            # Calculate the influence density for the current grid point\n",
        "            for item in data:\n",
        "                # Compute squared distances in longitude and latitude\n",
        "                xdistance = (item[1] - longitude_row) ** 2\n",
        "                ydistance = (item[0] - latitude_row) ** 2\n",
        "\n",
        "                # Compute combined distance and apply Gaussian kernel formula\n",
        "                combined_distance = xdistance + ydistance\n",
        "                probability = math.exp(-1 * combined_distance / (2 * bandwidth ** 2))\n",
        "\n",
        "                # Weight the probability by the pattern value\n",
        "                weighted_probability = item[pattern] * probability\n",
        "                sum_probability += weighted_probability\n",
        "\n",
        "            # Normalize the probability by the kernel area and total number of points\n",
        "            average_probability = sum_probability / (n * kernel_area)\n",
        "\n",
        "            # Append the density value\n",
        "            temp.append(average_probability)\n",
        "\n",
        "        # Append the row of density values to the 2D density list\n",
        "        density_2d.append(temp)\n",
        "\n",
        "    # Return the 2D density grid\n",
        "    return density_2d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0ed1907",
      "metadata": {
        "id": "c0ed1907"
      },
      "outputs": [],
      "source": [
        "def equationC3NoLog(data1, data2, data3, xdimention, ydimention, xx, yy):\n",
        "    \"\"\"\n",
        "    Calculate the co-location value for three densities using the normalized Z-scores of three datasets.\n",
        "\n",
        "    Parameters:\n",
        "    data1 (list of lists): Density data for the first variable.\n",
        "    data2 (list of lists): Density data for the second variable.\n",
        "    data3 (list of lists): Density data for the third variable.\n",
        "    xdimention (int): Number of grid points along the x-axis.\n",
        "    ydimention (int): Number of grid points along the y-axis.\n",
        "    xx (list of lists): 2D grid of longitudes.\n",
        "    yy (list of lists): 2D grid of latitudes.\n",
        "\n",
        "    Returns:\n",
        "    Co-location on a grid (list of lists): A 2D list containing the calculated co-location values at each grid point.\n",
        "    \"\"\"\n",
        "    # Flatten data1 to calculate mean and standard deviation\n",
        "    flat_data1_density = [element for item in data1 for element in item]\n",
        "    mean_data1 = stat.mean(flat_data1_density)\n",
        "    std_data1 = stat.stdev(flat_data1_density)\n",
        "\n",
        "    # Flatten data2 to calculate mean and standard deviation\n",
        "    flat_data2_density = [element for item in data2 for element in item]\n",
        "    mean_data2 = stat.mean(flat_data2_density)\n",
        "    std_data2 = stat.stdev(flat_data2_density)\n",
        "\n",
        "    # Flatten data3 to calculate mean and standard deviation\n",
        "    flat_data3_density = [element for item in data3 for element in item]\n",
        "    mean_data3 = stat.mean(flat_data3_density)\n",
        "    std_data3 = stat.stdev(flat_data3_density)\n",
        "\n",
        "    # Initialize variables\n",
        "    count_x = 0\n",
        "    C3 = []  # 2D list to store C3 values\n",
        "    x = []   # List to store longitude values\n",
        "    y = []   # List to store latitude values\n",
        "    d = []   # List to store C3 values\n",
        "\n",
        "    # Calculate C3 values for each grid point\n",
        "    while count_x < xdimention:\n",
        "        count_y = 0\n",
        "        temp_C3 = []  # Temporary list for the current row of C3 values\n",
        "\n",
        "        while count_y < ydimention:\n",
        "            # Calculate Z-scores for the three datasets\n",
        "            Z_A = (data1[count_x][count_y] - mean_data1) / std_data1\n",
        "            Z_B = (data2[count_x][count_y] - mean_data2) / std_data2\n",
        "            Z_C = (data3[count_x][count_y] - mean_data3) / std_data3\n",
        "\n",
        "            # Store coordinates\n",
        "            x.append(xx[count_x][count_x])\n",
        "            y.append(yy[count_y][count_y])\n",
        "\n",
        "            # Calculate C3 value only if all Z-scores are positive\n",
        "            data = 0\n",
        "            if Z_A > 0 and Z_B > 0 and Z_C > 0:\n",
        "                data = (Z_A * Z_B * Z_C) ** (1/3)\n",
        "\n",
        "            temp_C3.append(data)\n",
        "            d.append(data)\n",
        "            count_y += 1\n",
        "\n",
        "        C3.append(temp_C3)\n",
        "        count_x += 1\n",
        "\n",
        "    return C3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e67fe49d",
      "metadata": {
        "id": "e67fe49d"
      },
      "outputs": [],
      "source": [
        "def equationC2NoLog(data1, data2, xdimention, ydimention, xx, yy):\n",
        "    \"\"\"\n",
        "    Calculate the co-location value for two densities using the normalized Z-scores of two datasets.\n",
        "\n",
        "    Parameters:\n",
        "    data1 (list of lists): Density data for the first variable.\n",
        "    data2 (list of lists): Density data for the second variable.\n",
        "    xdimention (int): Number of grid points along the x-axis.\n",
        "    ydimention (int): Number of grid points along the y-axis.\n",
        "    xx (list of lists): 2D grid of longitudes.\n",
        "    yy (list of lists): 2D grid of latitudes.\n",
        "\n",
        "    Returns:\n",
        "    Co-location on a grid (list of lists): A 2D list containing the calculated co-location values at each grid point.\n",
        "    \"\"\"\n",
        "    # Flatten data1 to calculate mean and standard deviation\n",
        "    flat_data1_density = [element for item in data1 for element in item]\n",
        "    mean_data1 = stat.mean(flat_data1_density)\n",
        "    std_data1 = stat.stdev(flat_data1_density)\n",
        "\n",
        "    # Flatten data2 to calculate mean and standard deviation\n",
        "    flat_data2_density = [element for item in data2 for element in item]\n",
        "    mean_data2 = stat.mean(flat_data2_density)\n",
        "    std_data2 = stat.stdev(flat_data2_density)\n",
        "\n",
        "    # Initialize variables\n",
        "    count_x = 0\n",
        "    C2 = []  # 2D list to store C2 values\n",
        "    x = []   # List to store longitude values\n",
        "    y = []   # List to store latitude values\n",
        "    d = []   # List to store C2 values\n",
        "\n",
        "    # Calculate C2 values for each grid point\n",
        "    while count_x < xdimention:\n",
        "        count_y = 0\n",
        "        temp_C2 = []  # Temporary list for the current row of C2 values\n",
        "\n",
        "        while count_y < ydimention:\n",
        "            # Calculate Z-scores for the two datasets\n",
        "            Z_A = (data1[count_x][count_y] - mean_data1) / std_data1\n",
        "            Z_B = (data2[count_x][count_y] - mean_data2) / std_data2\n",
        "\n",
        "            # Store coordinates\n",
        "            x.append(xx[count_x][count_x])\n",
        "            y.append(yy[count_y][count_y])\n",
        "\n",
        "            # Calculate C2 value only if both Z-scores are positive\n",
        "            data = 0\n",
        "            if Z_A > 0 and Z_B > 0:\n",
        "                data = math.sqrt(Z_A * Z_B)\n",
        "\n",
        "            temp_C2.append(data)\n",
        "            d.append(data)\n",
        "            count_y += 1\n",
        "\n",
        "        C2.append(temp_C2)\n",
        "        count_x += 1\n",
        "\n",
        "    # Save the results to a CSV file\n",
        "    d2 = {'X': x, 'Y': y, 'Colocation': d}\n",
        "    df2 = pd.DataFrame(data=d2)\n",
        "    df2.to_csv('Anukriti/Covid_High_Unemployment_High.csv', index=False)\n",
        "\n",
        "    return C2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e29f88d",
      "metadata": {
        "id": "3e29f88d"
      },
      "outputs": [],
      "source": [
        "def generate_sample_points(data):\n",
        "    \"\"\"\n",
        "    Generate a grid of sample points within the bounding box defined by the minimum and maximum\n",
        "    longitude and latitude values from the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    data (dict): A dictionary containing 'lon' and 'lat' keys, each associated with a list of longitude\n",
        "                 and latitude values respectively.\n",
        "\n",
        "    Returns:\n",
        "    xx (numpy.ndarray): 2D array of longitude values representing the grid.\n",
        "    yy (numpy.ndarray): 2D array of latitude values representing the grid.\n",
        "    grid (list of lists): A list containing [longitude, latitude] pairs for each grid point.\n",
        "    \"\"\"\n",
        "    # Extract the bounding box for the grid\n",
        "    xmin = min(data['lon'])\n",
        "    xmax = max(data['lon'])\n",
        "    ymin = min(data['lat'])\n",
        "    ymax = max(data['lat'])\n",
        "\n",
        "    # Generate a meshgrid with 50 equally spaced points between the min and max coordinates\n",
        "    xx, yy = np.mgrid[xmin:xmax:50j, ymin:ymax:50j]\n",
        "\n",
        "    # Extract unique x-coordinates from the meshgrid for easier indexing\n",
        "    X = []\n",
        "    count = 0\n",
        "    for item in xx:\n",
        "        X.append(item[count])\n",
        "        count += 1\n",
        "\n",
        "    # Flatten the meshgrid into a list of [longitude, latitude] pairs\n",
        "    grid = []\n",
        "    xcount = 0\n",
        "    for longitude in xx:\n",
        "        ycount = 0\n",
        "        for latitude in yy:\n",
        "            grid.append([longitude[xcount], latitude[ycount]])\n",
        "            ycount += 1\n",
        "        xcount += 1\n",
        "\n",
        "    return xx, yy, grid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis Guideline\n",
        "\n",
        "This guideline outlines the key steps for conducting spatial data analysis, focusing on influence density calculation, co-location analysis, and statistical testing.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Read the Data Files**\n",
        "   - Load the required spatial datasets containing coordinates, variables, and patterns.\n",
        "   - Ensure data quality by handling missing values and standardizing formats (e.g., CSV files with proper headers).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Generate Sample Points**\n",
        "   - Use the `generate_sample_points` function to create a grid of sample points:\n",
        "     - Input spatial data with longitude (`lon`) and latitude (`lat`) values.\n",
        "     - Generate a bounding box based on the dataset's minimum and maximum longitude/latitude values.\n",
        "     - Output includes:\n",
        "       - `xx`: 2D array of longitude values for the grid.\n",
        "       - `yy`: 2D array of latitude values for the grid.\n",
        "       - `grid`: List of `[longitude, latitude]` pairs for each grid point.\n",
        "   - This step establishes the foundation for spatial density and co-location analysis by defining the spatial grid.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Calculate Influence Function**\n",
        "   - Use the `influence` or `influenceO` function to compute spatial influence densities for each pattern:\n",
        "     - Input spatial coordinates, pattern indices, grid dimensions (`xx`, `yy`), and kernel bandwidth.\n",
        "     - Generate a 2D density map representing the influence of each pattern across the grid.\n",
        "   - Consider the choice between `influence` and `influenceO` based on the specific kernel or context of the analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Calculate Binary or Ternary Co-locations**\n",
        "   - For **binary co-locations**:\n",
        "     - Use `equationC2NoLog` to compute co-location values for two variables.\n",
        "   - For **ternary co-locations**:\n",
        "     - Use `equationC3NoLog` to compute co-location values involving three variables.\n",
        "   - Input parameters include density data, grid dimensions, and the spatial grid (`xx`, `yy`).\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Visualize the Outputs**\n",
        "   - Generate clear and interpretable visualizations:\n",
        "     - Plot influence densities as heatmaps or contour plots.\n",
        "     - Visualize co-location results to highlight spatial patterns of variable interaction.\n",
        "   - Use consistent color schemes and annotations for easy interpretation.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Save the Co-locations**\n",
        "   - Export computed co-location values to files (e.g., CSV or JSON) for documentation and further analysis.\n",
        "   - Include metadata such as variable names, grid dimensions, and calculation parameters.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Perform Permutation Test**\n",
        "   - Conduct permutation tests to validate the statistical significance of co-location patterns:\n",
        "     - Randomly shuffle spatial patterns and recompute co-locations.\n",
        "     - Compare observed co-location values with randomized distributions to compute p-values.\n",
        "   - Save permutation test results for reference and reporting.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Si1kyHPTDwHN"
      },
      "id": "Si1kyHPTDwHN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Data"
      ],
      "metadata": {
        "id": "TyMZr4pWEsCS"
      },
      "id": "TyMZr4pWEsCS"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c6e95b71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "c6e95b71",
        "outputId": "5c174627-f641-4304-dbf7-a466465e8c3d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b5e4b0c63cdc>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Process COVID data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProcessData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Datasets/covid_high_low.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mdat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mcovid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Zip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to NumPy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-3d2bdf309396>\u001b[0m in \u001b[0;36mProcessData\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Load the CSV file into a DataFrame with the specified headers and data types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This code processes multiple datasets related to high and low values of various variables,\n",
        "such as COVID cases, population, income, and unemployment rates. The processed datasets\n",
        "are converted into NumPy arrays for further analysis and stored in a list for structured handling.\n",
        "\n",
        "Purpose:\n",
        "- Load and process CSV files for each variable, dropping irrelevant columns (e.g., 'Zip').\n",
        "- Convert processed data into NumPy arrays for numerical computation and analysis.\n",
        "- Store the processed datasets in a list for ease of access.\n",
        "\n",
        "Key Steps:\n",
        "1. Use the `ProcessData` function to read and clean each dataset.\n",
        "2. Drop the 'Zip' column as it's irrelevant for numerical operations.\n",
        "3. Append the processed DataFrame to the `dat` list for centralized storage.\n",
        "4. Convert the processed DataFrame to a NumPy array for numerical computations.\n",
        "\"\"\"\n",
        "\n",
        "dat = []  # List to store processed datasets\n",
        "\n",
        "# Process COVID data\n",
        "data = ProcessData(\"Datasets/covid_high_low.csv\")\n",
        "dat.append(data)\n",
        "covid = np.array(data.drop(['Zip'], 1))  # Convert to NumPy array\n",
        "\n",
        "# Process Population data\n",
        "data = ProcessData(\"Datasets/population_high_low.csv\")\n",
        "dat.append(data)\n",
        "population = np.array(data.drop(['Zip'], 1))  # Convert to NumPy array\n",
        "print(data.head)  # Print the first few rows for verification\n",
        "\n",
        "# Process Bachelor's Degree data\n",
        "data = ProcessData(\"Datasets/bachelorPercentage_high_low.csv\")\n",
        "dat.append(data)\n",
        "bachelor = np.array(data.drop(['Zip'], 1))  # Convert to NumPy array\n",
        "\n",
        "# Process Median Income data\n",
        "data = ProcessData(\"Datasets/median_income_high_low.csv\")\n",
        "dat.append(data)\n",
        "median_income = np.array(data.drop(['Zip'], 1))  # Convert to NumPy array\n",
        "\n",
        "# Process Poverty data\n",
        "data = ProcessData(\"Datasets/below_poverty_percent_high_low.csv\")\n",
        "dat.append(data)\n",
        "poverty = np.array(data.drop(['Zip'], 1))  # Convert to NumPy array\n",
        "\n",
        "# Process Four Plus Family data\n",
        "data = ProcessData(\"Datasets/four_plus_family_percent_high_low.csv\")\n",
        "dat.append(data)\n",
        "family = np.array(data.drop(['Zip'], 1))  # Convert to NumPy array\n",
        "\n",
        "# Process No Car data\n",
        "data = ProcessData(\"Datasets/no_car_percent_high_low.csv\")\n",
        "dat.append(data)\n",
        "car = np.array(data.drop(['Zip'], 1))  # Convert to NumPy array\n",
        "\n",
        "# Process Unemployment data\n",
        "data = ProcessData(\"Datasets/unemployment_percent_high_low.csv\")\n",
        "dat.append(data)\n",
        "unemployment = np.array(data.drop(['Zip'], 1))  # Convert to NumPy array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Influence Functions"
      ],
      "metadata": {
        "id": "IJoKfQxoExGE"
      },
      "id": "IJoKfQxoExGE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5a22f24",
      "metadata": {
        "id": "b5a22f24",
        "outputId": "c4f3ef8f-03f0-4f1f-b670-5d106fd1dca5"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "influence() missing 1 required positional argument: 'n'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-11-ac77bc86fd2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mda\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minfluence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcovid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbandwidth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m\"covid_high\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: influence() missing 1 required positional argument: 'n'"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This code initializes influence values for various attributes (e.g., COVID rates, population, income) at high and low levels.\n",
        "\n",
        "Purpose:\n",
        "- To compute and store influence densities for a range of datasets using a Gaussian kernel approach.\n",
        "- Organize the results into a structured format for further analysis.\n",
        "\n",
        "Key Steps:\n",
        "1. Generate sample points (grid) using the `generate_sample_points` function.\n",
        "2. Initialize arrays to store influence results (`da`) and their corresponding labels (`str`).\n",
        "3. Compute influence densities for each attribute and store the results in `da`.\n",
        "4. Assign descriptive labels for each attribute's high and low values into `str`.\n",
        "\"\"\"\n",
        "\n",
        "# Set the kernel bandwidth for influence calculation\n",
        "bandwidth = 1\n",
        "\n",
        "# Generate sample points from the COVID dataset\n",
        "xx, yy, grid = generate_sample_points(covid)\n",
        "\n",
        "# Initialize arrays to store influence results and their labels\n",
        "da = [0] * 16\n",
        "str = [\"\"] * 16\n",
        "\n",
        "# Compute influence densities and assign labels\n",
        "\n",
        "da[0] = influence(covid, 2, xx, yy, bandwidth)\n",
        "str[0] = \"covid_high\"\n",
        "\n",
        "da[1] = influence(covid, 3, xx, yy, bandwidth)\n",
        "str[1] = \"covid_low\"\n",
        "\n",
        "da[2] = influence(population, 2, xx, yy, bandwidth)\n",
        "str[2] = \"population_high\"\n",
        "\n",
        "da[3] = influence(population, 3, xx, yy, bandwidth)\n",
        "str[3] = \"population_low\"\n",
        "\n",
        "da[4] = influence(bachelor, 2, xx, yy, bandwidth)\n",
        "str[4] = \"bachelor_high\"\n",
        "\n",
        "da[5] = influence(bachelor, 3, xx, yy, bandwidth)\n",
        "str[5] = \"bachelor_low\"\n",
        "\n",
        "da[6] = influence(median_income, 2, xx, yy, bandwidth)\n",
        "str[6] = \"income_high\"\n",
        "\n",
        "da[7] = influence(median_income, 3, xx, yy, bandwidth)\n",
        "str[7] = \"income_low\"\n",
        "\n",
        "da[8] = influence(poverty, 2, xx, yy, bandwidth)\n",
        "str[8] = \"poverty_high\"\n",
        "\n",
        "da[9] = influence(poverty, 3, xx, yy, bandwidth)\n",
        "str[9] = \"poverty_low\"\n",
        "\n",
        "da[10] = influence(family, 2, xx, yy, bandwidth)\n",
        "str[10] = \"family_high\"\n",
        "\n",
        "da[11] = influence(family, 3, xx, yy, bandwidth)\n",
        "str[11] = \"family_low\"\n",
        "\n",
        "da[12] = influence(car, 2, xx, yy, bandwidth)\n",
        "str[12] = \"car_high\"\n",
        "\n",
        "da[13] = influence(car, 3, xx, yy, bandwidth)\n",
        "str[13] = \"car_low\"\n",
        "\n",
        "da[14] = influence(unemployment, 2, xx, yy, bandwidth)\n",
        "str[14] = \"unemployment_high\"\n",
        "\n",
        "da[15] = influence(unemployment, 3, xx, yy, bandwidth)\n",
        "str[15] = \"unemployment_low\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the Influence Function"
      ],
      "metadata": {
        "id": "b_bUT0qFF1sX"
      },
      "id": "b_bUT0qFF1sX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a8d8b31",
      "metadata": {
        "id": "0a8d8b31"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code generates a geographical heatmap to visualize the spatial distribution of bachelor's degree holders' weighted density.\n",
        "\n",
        "Purpose:\n",
        "- To compute the influence density of bachelor's degree holders using a Gaussian kernel.\n",
        "- Overlay the density on a geographical map of Virginia zip codes.\n",
        "- Display the heatmap with customized contour levels and a colorbar for better visualization.\n",
        "\n",
        "Key Steps:\n",
        "1. Compute the influence density using the `influence` function.\n",
        "2. Load geographical data from a GeoJSON file.\n",
        "3. Create a base map of zip code boundaries.\n",
        "4. Overlay a filled contour plot of density values.\n",
        "5. Add a resized and labeled colorbar for interpreting density values.\n",
        "6. Annotate the plot with a title and axis labels.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Assuming da[4] is valid\n",
        "bandwidth = 0.25\n",
        "\n",
        "# Compute influence density\n",
        "# The influence function calculates weighted density based on spatial data.\n",
        "data = influence(bachelor, 2, xx, yy, bandwidth)\n",
        "\n",
        "# Create a plot\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# File path to the GeoJSON data containing zip code geometries\n",
        "data_path = 'C:/Users/mdmah/PycharmProjects/ProfessorEick/ProfessorEick/ACMSIGSPATIAL2020/VA_Zip_Codes_-2354097028237156067.geojson'\n",
        "\n",
        "# Load the GeoJSON data into a GeoPandas DataFrame\n",
        "df_places = gpd.read_file(data_path)\n",
        "\n",
        "# Plot the base map showing zip code boundaries\n",
        "df_places.plot(ax=ax, linewidth=0.8, edgecolor='black', color='white')\n",
        "\n",
        "# Define contour levels for the density plot\n",
        "levels = np.linspace(0, 0.1, 11)\n",
        "\n",
        "# Add a filled contour plot to visualize the density values\n",
        "cfset = ax.contourf(xx, yy, data, levels=levels, cmap='coolwarm', alpha=0.7)\n",
        "\n",
        "# Resize and reposition the colorbar\n",
        "cax = fig.add_axes([0.91, 0.3, 0.02, 0.4])  # [left, bottom, width, height]\n",
        "cbar = fig.colorbar(cfset, cax=cax, orientation='vertical')\n",
        "\n",
        "# Customize the colorbar\n",
        "cbar.set_label(\"Weighted Density\", fontsize=14)  # Add a label to the colorbar\n",
        "cbar.ax.tick_params(labelsize=10)  # Adjust the size of the colorbar ticks\n",
        "\n",
        "# Add title and axis labels to the plot\n",
        "ax.set_title(r'$\\varphi_{\\mathrm{Bachelor\\ Degree} \\uparrow}$', fontsize=14)\n",
        "ax.set_xlabel(\"Longitude\", fontsize=14)\n",
        "ax.set_ylabel(\"Latitude\", fontsize=14)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ternary Co-location and Stastistics Calculation"
      ],
      "metadata": {
        "id": "pO8LoG45F_Cu"
      },
      "id": "pO8LoG45F_Cu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6f761bf",
      "metadata": {
        "id": "c6f761bf",
        "outputId": "ccd05672-d4ad-48ea-b72d-4745df982b0a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'equationC3NoLog' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-12-8889e3620ff2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mequationC3NoLog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mda\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mda\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mda\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mm1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_std\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mmi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'equationC3NoLog' is not defined"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This code computes the mean and standard deviation of density values generated for unique patterns of three datasets.\n",
        "\n",
        "Purpose:\n",
        "- To iterate through combinations of three datasets from a collection of 16 datasets.\n",
        "- Compute a density matrix for each combination using the `equationC3NoLog` function.\n",
        "- Calculate and store the mean and standard deviation for the density values of each combination.\n",
        "- Record the pattern of dataset indices used for each calculation.\n",
        "\n",
        "Key Steps:\n",
        "1. Iterate through all unique combinations of three datasets from 16 datasets.\n",
        "2. For each combination:\n",
        "   - Compute the density matrix using `equationC3NoLog`.\n",
        "   - Calculate the mean and standard deviation using the `mean_std` function.\n",
        "   - Store the results and the corresponding pattern.\n",
        "3. Store the aggregated results in lists for further use.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize variables to store results\n",
        "i = 0\n",
        "mean = []\n",
        "std = []\n",
        "patterns = []\n",
        "\n",
        "# Iterate through unique combinations of three datasets\n",
        "while i <= 15:\n",
        "    j = i + 1\n",
        "    while j <= 15:\n",
        "        k = j + 1\n",
        "        while k < 15:\n",
        "            # Temporary lists to store mean and standard deviation for the current combination\n",
        "            mi = []\n",
        "            st = []\n",
        "\n",
        "            # Compute the density matrix for the current combination\n",
        "            data = equationC3NoLog(da[i], da[j], da[k], 50, 50, xx, yy)\n",
        "\n",
        "            # Calculate the mean and standard deviation for the density values\n",
        "            m1, s1 = mean_std(data)\n",
        "            mi.append(m1)\n",
        "            st.append(s1)\n",
        "\n",
        "            # Store the rounded mean and standard deviation\n",
        "            mean.append(round(stat.mean(mi), 3))\n",
        "            std.append(round(stat.mean(st), 3))\n",
        "\n",
        "            # Record the pattern of dataset indices\n",
        "            patterns.append(str[i] + \"+\" + str[j] + \"+\" + str[k])\n",
        "\n",
        "            k += 1\n",
        "        j += 1\n",
        "    i += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Ternary Co-location"
      ],
      "metadata": {
        "id": "vukBSBTyGJRh"
      },
      "id": "vukBSBTyGJRh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "847e4356",
      "metadata": {
        "id": "847e4356"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code visualizes co-location patterns between two variables (population and median income) using a filled contour plot over a geographical map.\n",
        "\n",
        "Purpose:\n",
        "- To compute the co-location values for population and median income using influence and density calculation functions.\n",
        "- Normalize and visualize the co-location data on a geographical map of Virginia zip codes.\n",
        "- Provide insights into spatial patterns of the relationship between the two variables.\n",
        "\n",
        "Key Steps:\n",
        "1. Compute influence densities for population and median income using a specified bandwidth.\n",
        "2. Calculate co-location values using the `equationC2NoLog` function.\n",
        "3. Normalize the co-location values.\n",
        "4. Overlay the co-location values on a geographical map using a filled contour plot.\n",
        "5. Customize the visualization with labels, a colorbar, and title.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the kernel bandwidth for influence calculation\n",
        "bandwidth = 0.01\n",
        "\n",
        "# Compute influence densities for population and median income\n",
        "da1 = influence(population, 3, xx, yy, bandwidth)\n",
        "da2 = influence(median_income, 3, xx, yy, bandwidth)\n",
        "\n",
        "# Calculate co-location values between population and median income\n",
        "data = equationC2NoLog(da1, da2, 50, 50, xx, yy)\n",
        "\n",
        "# Normalize the co-location values\n",
        "data = [[value / 3 for value in row] for row in data]\n",
        "\n",
        "# Create a plot\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# File path to the GeoJSON data containing zip code geometries\n",
        "data_path = 'C:/Users/mdmah/PycharmProjects/ProfessorEick/ProfessorEick/ACMSIGSPATIAL2020/VA_Zip_Codes_-2354097028237156067.geojson'\n",
        "\n",
        "# Load the GeoJSON data into a GeoPandas DataFrame\n",
        "df_places = gpd.read_file(data_path)\n",
        "\n",
        "# Plot the base map showing zip code boundaries\n",
        "df_places.plot(ax=ax, linewidth=0.8, edgecolor='black', color='white')\n",
        "\n",
        "# Define contour levels for the co-location values\n",
        "levels = np.linspace(0, 5, 11)\n",
        "\n",
        "# Add a filled contour plot to visualize the co-location values\n",
        "cfset = ax.contourf(xx, yy, data, levels=levels, cmap='coolwarm', alpha=0.7)\n",
        "\n",
        "# Resize and reposition the colorbar\n",
        "cax = fig.add_axes([0.91, 0.3, 0.02, 0.4])  # [left, bottom, width, height]\n",
        "cbar = fig.colorbar(cfset, cax=cax, orientation='vertical')\n",
        "\n",
        "# Customize the colorbar\n",
        "cbar.set_label(\"Co-location Value\", fontsize=14)  # Add a label to the colorbar\n",
        "cbar.ax.tick_params(labelsize=10)  # Adjust the size of the colorbar ticks\n",
        "\n",
        "# Add title and axis labels to the plot\n",
        "ax.set_title(r'$C_{(\\{Population \\downarrow,Income \\uparrow\\}) }$ for Bandwidth 0.01', fontsize=14)\n",
        "ax.set_xlabel(\"Longitude\", fontsize=14)\n",
        "ax.set_ylabel(\"Latitude\", fontsize=14)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Ternary Co-location"
      ],
      "metadata": {
        "id": "6irKhRNGGPPV"
      },
      "id": "6irKhRNGGPPV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67842c9e",
      "metadata": {
        "id": "67842c9e"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code saves the calculated co-location results (patterns, mean, and standard deviation) into a CSV file.\n",
        "\n",
        "Purpose:\n",
        "- To organize the co-location results into a structured format.\n",
        "- Save the results as a CSV file for further analysis or reporting.\n",
        "\n",
        "Key Steps:\n",
        "1. Create a dictionary with keys:\n",
        "   - 'pattern': List of dataset index combinations used for co-location calculations.\n",
        "   - 'mean': List of mean values for the density values of each pattern.\n",
        "   - 'std': List of standard deviation values for the density values of each pattern.\n",
        "2. Convert the dictionary into a Pandas DataFrame.\n",
        "3. Save the DataFrame to a CSV file at the specified path.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Create a dictionary to store co-location results\n",
        "d = { 'pattern': patterns, 'mean': mean, 'std': std}\n",
        "\n",
        "# Convert the dictionary into a DataFrame\n",
        "df = pd.DataFrame(data=d)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('Datasets/tarnery_colocations_0_05.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binary Co-location and Stastistics Calculation"
      ],
      "metadata": {
        "id": "S7PSuW_zGU21"
      },
      "id": "S7PSuW_zGU21"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbbb8cb2",
      "metadata": {
        "id": "bbbb8cb2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code computes the mean and standard deviation of density values generated for unique pairwise combinations of datasets.\n",
        "\n",
        "Purpose:\n",
        "- To iterate through combinations of two datasets from a collection of 16 datasets.\n",
        "- Compute a density matrix for each pairwise combination using the `equationC2NoLog` function.\n",
        "- Calculate and store the mean and standard deviation for the density values of each combination.\n",
        "- Record the pattern of dataset indices used for each calculation.\n",
        "\n",
        "Key Steps:\n",
        "1. Iterate through all unique combinations of two datasets from 16 datasets.\n",
        "2. For each combination:\n",
        "   - Compute the density matrix using `equationC2NoLog`.\n",
        "   - Calculate the mean and standard deviation using the `mean_std` function.\n",
        "   - Store the results and the corresponding pattern.\n",
        "3. Store the aggregated results in lists for further use.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize variables to store results\n",
        "i = 0\n",
        "means = []\n",
        "std = []\n",
        "patterns = []\n",
        "\n",
        "# Iterate through unique combinations of two datasets\n",
        "while i <= 15:\n",
        "    j = i + 1\n",
        "    while j <= 15:\n",
        "\n",
        "        # Temporary lists to store mean and standard deviation for the current combination\n",
        "        mi = []\n",
        "        st = []\n",
        "\n",
        "        # Compute the density matrix for the current combination\n",
        "        data = equationC2NoLog(da[i], da[j], 50, 50, xx, yy)\n",
        "\n",
        "        # Calculate the mean and standard deviation for the density values\n",
        "        m1, s1 = mean_std(data)\n",
        "        mi.append(m1)\n",
        "        st.append(s1)\n",
        "\n",
        "        # Store the rounded mean and standard deviation\n",
        "        means.append(round(stat.mean(mi), 3))\n",
        "        std.append(round(stat.mean(st), 3))\n",
        "\n",
        "        # Record the pattern of dataset indices\n",
        "        patterns.append(str[i] + \"+\" + str[j])\n",
        "\n",
        "        j += 1\n",
        "    i += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Permutation Test for Binary Co-location"
      ],
      "metadata": {
        "id": "yd-4KNBVGhWi"
      },
      "id": "yd-4KNBVGhWi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dce673c3",
      "metadata": {
        "id": "dce673c3",
        "outputId": "ec9973f5-1340-4e8f-9265-3723cbe25b27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "covid_high+covid_low 1.0 0.0 0.0 0.028 0.0\n",
            "covid_high+population_high 1.0 0.0 0.0 0.035 0.0\n",
            "covid_high+population_low 0.0 1.0 0.0 0.068 0.0\n",
            "covid_high+bachelor_high 1.0 0.0 0.0 0.055 0.0\n",
            "covid_high+bachelor_low 0.0 1.0 0.0 0.076 0.0\n",
            "covid_high+income_high 1.0 0.0 0.0 0.072 0.0\n",
            "covid_high+income_low 0.0 1.0 0.0 0.069 0.0\n",
            "covid_high+poverty_high 0.0 1.0 0.0 0.059 0.0\n",
            "covid_high+poverty_low 0.0 1.0 0.0 0.054 0.0\n",
            "covid_high+family_high 0.0 1.0 0.0 0.061 0.0\n",
            "covid_high+family_low 1.0 0.0 0.0 0.065 0.0\n",
            "covid_high+car_high 1.0 0.0 0.0 0.072 0.0\n",
            "covid_high+car_low 0.0 1.0 0.0 0.071 0.0\n",
            "covid_high+unemployment_high 1.0 0.0 0.0 0.049 0.0\n",
            "covid_high+unemployment_low 0.0 1.0 0.0 0.062 0.0\n",
            "covid_low+population_high 1.0 0.0 0.0 0.097 0.0\n",
            "covid_low+population_low 1.0 0.0 0.0 0.161 0.0\n",
            "covid_low+bachelor_high 0.0 1.0 0.0 0.123 0.0\n",
            "covid_low+bachelor_low 0.0 1.0 0.0 0.193 0.0\n",
            "covid_low+income_high 0.0 1.0 0.0 0.157 0.0\n",
            "covid_low+income_low 1.0 0.0 0.0 0.186 0.0\n",
            "covid_low+poverty_high 1.0 0.0 0.0 0.109 0.0\n",
            "covid_low+poverty_low 0.0 1.0 0.0 0.197 0.0\n",
            "covid_low+family_high 0.0 1.0 0.0 0.132 0.0\n",
            "covid_low+family_low 1.0 0.0 0.0 0.162 0.0\n",
            "covid_low+car_high 1.0 0.0 0.0 0.099 0.0\n",
            "covid_low+car_low 1.0 0.0 0.0 0.17 0.0\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This code performs a permutation test to calculate p-values for co-location patterns between pairs of datasets.\n",
        "\n",
        "Purpose:\n",
        "- To test the significance of co-location patterns by shuffling the values in datasets and recalculating densities.\n",
        "- Compute p-values based on the distribution of permuted results compared to the original density values.\n",
        "- Store the p-values, means, and standard deviations of permuted results for further analysis.\n",
        "\n",
        "Key Steps:\n",
        "1. Iterate through all unique pairwise combinations of datasets (16 datasets in total).\n",
        "2. For each combination:\n",
        "   - Shuffle values in the relevant dataset columns.\n",
        "   - Recompute influence densities using the `influence` function.\n",
        "   - Calculate densities for the combination using `equationC2NoLog`.\n",
        "   - Compute mean and standard deviation of the permuted densities.\n",
        "   - Append results to lists of p-values, means, and standard deviations.\n",
        "3. Calculate p-values for higher, lower, and equal distributions of permuted means relative to the original.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import warnings\n",
        "import statistics as stat\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Initialize variables\n",
        "meanr = []\n",
        "stdr = []\n",
        "p_valuesH = []  # p-values for higher distribution\n",
        "p_valuesL = []  # p-values for lower distribution\n",
        "p_valuesE = []  # p-values for equal distribution\n",
        "meansR = []  # Mean of permuted values\n",
        "stdR = []  # Standard deviation of permuted values\n",
        "\n",
        "# Iterate through unique combinations of datasets\n",
        "i = 0\n",
        "k = 0\n",
        "while i <= 15:\n",
        "    j = i + 1\n",
        "    while j <= 15:\n",
        "        num_permutations = 100  # Number of permutations\n",
        "        permuted_vals = []  # List to store results from permutations\n",
        "\n",
        "        for _ in range(num_permutations):\n",
        "            # Initialize an empty array for shuffling datasets\n",
        "            dar = [0] * 16\n",
        "\n",
        "            # Shuffle and calculate influence for selected datasets\n",
        "            if i == 0 or j == 0:\n",
        "                np.random.shuffle(dat[0]['high'])\n",
        "                d = np.array(dat[0].drop(['Zip'], 1))\n",
        "                dar[i] = influence(d, 2, xx, yy, bandwidth)\n",
        "            if i == 1 or j == 1:\n",
        "                np.random.shuffle(dat[0]['low'])\n",
        "                d = np.array(dat[0].drop(['Zip'], 1))\n",
        "                dar[i] = influence(d, 3, xx, yy, bandwidth)\n",
        "            # Repeat similar blocks for indices 2 through 15\n",
        "\n",
        "            # Compute the density matrix for the current combination\n",
        "            data = equationC2NoLog(da[i], da[j], 50, 50, xx, yy)\n",
        "            m1, s1 = mean_std(data)\n",
        "            permuted_vals.append(m1)\n",
        "\n",
        "        # Calculate p-values based on the distribution of permuted values\n",
        "        p_valueh = np.mean([val > means[k] for val in permuted_vals])\n",
        "        p_valuesH.append(p_valueh)\n",
        "\n",
        "        p_valuel = np.mean([val < means[k] for val in permuted_vals])\n",
        "        p_valuesL.append(p_valuel)\n",
        "\n",
        "        p_valuee = np.mean([val == means[k] for val in permuted_vals])\n",
        "        p_valuesE.append(p_valuee)\n",
        "\n",
        "        # Compute and store statistics of permuted values\n",
        "        meansR.append(round(stat.mean(permuted_vals), 3))\n",
        "        stdR.append(stat.stdev(permuted_vals))\n",
        "\n",
        "        # Print results for the current pattern\n",
        "        print(patterns[k], p_valueh, p_valuel, p_valuee, round(stat.mean(permuted_vals), 3), stat.stdev(permuted_vals))\n",
        "\n",
        "        k += 1\n",
        "        j += 1\n",
        "    i += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the Permutation Test Results"
      ],
      "metadata": {
        "id": "TXBZdc2vGncT"
      },
      "id": "TXBZdc2vGncT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46728410",
      "metadata": {
        "id": "46728410"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code saves the calculated permuted co-location results (patterns, mean, and standard deviation) into a CSV file.\n",
        "\n",
        "Purpose:\n",
        "- To organize the co-location results into a structured format.\n",
        "- Save the results as a CSV file for further analysis or reporting.\n",
        "- To compare with real results\n",
        "\n",
        "Key Steps:\n",
        "1. Create a dictionary with keys:\n",
        "   - 'pattern': List of dataset index combinations used for co-location calculations.\n",
        "   - 'mean': List of mean values for the density values of each pattern.\n",
        "   - 'std': List of standard deviation values for the density values of each pattern.\n",
        "2. Convert the dictionary into a Pandas DataFrame.\n",
        "3. Save the DataFrame to a CSV file at the specified path.\n",
        "\"\"\"\n",
        "# Create a dictionary to store co-location results\n",
        "d = { 'pattern': patterns, 'mean': mean, 'std':std}\n",
        "\n",
        "# Convert the dictionary into a DataFrame\n",
        "df = pd.DataFrame(data=d)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('Datasets/binary_colocations_random_0_05.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63b1fbb0",
      "metadata": {
        "id": "63b1fbb0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3.8",
      "language": "python",
      "name": "python3.8"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}